---
title: Manual vs Automated Testing
abstract: 'Abstract first sentence.
          Abstract second sentence.
          Abstract third sentence.'
cover: /covers/image-file.png
author:
  - piotr.konkol
  - kamila.banecka
layout: post
published: true
date: 2021-02-05
archives: "2021"

tags:
  - testing
  - firmware
categories:
  - Firmware
  - IoT
  - Miscellaneous
  - OS Dev
  - App Dev
  - Security
  - Manufacturing

---

The Importance of test automation

It is common understanding that software, or – in our case – firmware must be
thoroughly tested before releasing it to the public. Without doing that we risk
leaving security vulnerabilities, user-experience destroying bugs, and less
optimal performance. While it may seem obvious what should be done, more
specific glimpse proves that there is more than one possible approach to
testing, and the choice however, clearly determines the further results. The
question is how testing should be performed, to provide the most valuable output
in the least amount of time, instead of clumsy and dangerous release. In this
blogpost we will try to narrow down the distinction to the core, as comparing
manual and automated testing. The former is the simpler method and does not add
the cost of writing code, so may be performed immediately. The later is based on
long term thinking about the problem, where scripts perform the tests without
human participation. And let's be clear – such investment into writing comes
with many further benefits.

How are manual tests different from automated tests? Manual testing is based on
performing the role of a standard user – no code is written. For obvious reasons,
the whole process takes much longer and, although initially it does not require
large investments, results in much lower ROI, due to the costs related to human
resources. In some cases, manual testing is an option worth considering, for
example when performing usability, or in no-scenario tests. However, in
reproducible tasks, like functional tests, only the automation will bring the
expected results.

Our validation infrastructure in 3mdeb consists of ~150 tests. To perform the
whole testing process step by step would take days, especially that the whole
process is repetitive. Summarizing, manual testing in this case costs more and
slows down the whole development pace, when great delay occurs between code
release and feedback for the programmers. If we would still try to test
manually, our knowledge of code quality would be worse and the delivery would be
delayed.

The whole process may be processed in a much more convenient way. Tests can be
run by the developer, who needs only a brief information about the testing
environment. Clear documentation and ready-to-use physical platforms with system
of automatic availability management make it possible to run both, the single
tests and complete suites, concerning a set of functionalities that must be
checked. In result we receive immediate feedback about the changes made and the
features that were added.

An automated testing infrastructure can be paired with constant integration, and
delivery pipeline, to enhance the overall performane. We successfully use such
approach when testing our projects. It all begins when most recent changes for
the release are ready in our core repository and are marked using tags as a
release. From this point the automatic process of building usable binaries
begins, signing it with authenticity proving keys, testing, and delivering the
final product to the customer, as an online ready-to-download file. Because
these time-consuming tasks are performing automatically, it leaves developers
more time for truly creative work on delivering the highest quality of product.

Test verification process needs understanding of the tested functionality and
further interpretation of the results. In case of automated testing, the output
provides result and definition for getting the necessary information for users,
clients and developers.

When the complete regression testing results for the given release are ready,
the next step is to make them accessible and easy to understand. The results are
the purest quality proof of the product deliverable. At this point, the task is
usually considered as closed, and the mentioned output is presented using public
spreadsheets. It serves the informing role, but this also can be improved and
automated. And this is what we did, we improved results publishing. While
waiting for our automated tests to be processed, we have created Dasharo
Transparent Validation. System, that automatically presents the results in a
clean way that is easy to get, and have many available filters to customize the
view for presenting the exact data needed, with support for charts embedabble on
a website or repository readme notes. 

## Summary

Summary of the post.

OPTIONAL ending (may be based on post content):

If you think we can help in improving the security of your firmware or you
looking for someone who can boost your product by leveraging advanced features
of used hardware platform, feel free to [book a call with us](https://calendly.com/3mdeb/consulting-remote-meeting)
or drop us email to `contact<at>3mdeb<dot>com`. If you are interested in similar
content feel free to [sign up to our newsletter](http://eepurl.com/doF8GX)
